{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SilvanaJ90/usergioarboleda-bootcamp_IA/blob/main/Tutorial_Funciones_Activacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puede revisar el siguiente material\n",
        "\n",
        "[Enlace](https://universidadsergioarboleda-my.sharepoint.com/:f:/g/personal/joaquin_sanchezc_usa_edu_co/EvD-PTlR2kZFnLTFkyvqOc4BniSw-L3DvPvmFF1svAK7Uw?e=9Wgpju)"
      ],
      "metadata": {
        "id": "NARyD9IHm8Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **¿Qué es una función de activación?**\n",
        "\n",
        "Es una función que se aplica a las neuronas de una capa durante la predicción.\n",
        "\n",
        "Una función de activación es una función que se aplica a las neuronas de una capa durante la predicción. Esto debería resultarte muy familiar, porque has estado utilizando una función de activación llamada relu (mostrada aquí en la red neuronal de tres capas). La función relu tenía el efecto de convertir todos los números negativos en 0.\n",
        "\n",
        "Simplificando demasiado, una función de activación es cualquier función que puede tomar un número y devolver otro número. Pero hay un número infinito de funciones en el universo, y no todas son útiles como funciones de activación.\n",
        "\n",
        "Hay varias restricciones sobre lo que hace que una función sea una función de activación. Usar funciones fuera de estas restricciones es normalmente una mala idea, como verás."
      ],
      "metadata": {
        "id": "Jkd0Vop6maCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo de Funciones de Activación\n",
        "\n",
        "* **Función de activación ReLU (Rectified Linear Unit):**\n",
        "Esta es una de las funciones de activación más utilizadas en redes neuronales. Es una función lineal rectificada y se define como $f(x)=max(0,x)$. Es simple y eficiente en términos de cómputo y ha demostrado ser efectiva en la mayoría de las aplicaciones.\n",
        "\n",
        "* **Función de activación Sigmoid:**\n",
        "La función sigmoide tiene una forma de \"S\" y es continua y diferenciable. Se define como $f(x)=\\frac{1}{1+e^{-x}}$\n",
        "Produce valores en el rango de 0 a 1 y es útil en la salida de modelos de clasificación binaria donde se desea una probabilidad de pertenencia a una clase.\n",
        "\n",
        "* **Función de activación Tangente Hiperbólica (Tanh):**\n",
        "La función tangente hiperbólica es similar a la función sigmoide pero produce valores en el rango de -1 a 1. Se define como $f(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$. Se utiliza a menudo en capas ocultas de redes neuronales.\n",
        "\n",
        "* **Función de activación Softmax:**\n",
        "La función softmax se utiliza comúnmente en la capa de salida de una red neuronal para problemas de clasificación multiclase. Transforma un vector de números reales en un vector de probabilidades que suman 1. Se define como: $f(x_{i})=\\frac{e^{x_{i}}}{\\sum{e^{x_{j}}}}$\n",
        "\n",
        "* **Función de activación Leaky ReLU:** La función Leaky ReLU es similar a ReLU pero permite un pequeño gradiente cuando $x<0$ lo que puede ayudar a resolver el problema de \"neuronas muertas\" Se define como $f(x)=max(ax,x)$, donde $a$ es un valor pequeño positivo.\n"
      ],
      "metadata": {
        "id": "jdgokddHxcwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Relu\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "#Función de activación Sigmoid:\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "#Función de activación Tangente Hiperbólica (Tanh):\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "#Función de activación Softmax:\n",
        "def softmax(x):\n",
        "    exp_values = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n",
        "\n",
        "#Función de activación Leaky ReLU:\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n"
      ],
      "metadata": {
        "id": "CK_cZGsl1vmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparación entre funciones de activación**\n",
        "\n",
        "En este ejemplo, utilizaremos un conjunto de datos de clasificación binaria y compararemos el rendimiento de las funciones de activación ReLU, Sigmoid y Tanh en la capa oculta de una red neuronal.\n",
        "\n"
      ],
      "metadata": {
        "id": "1dLcePsz2KZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Generar datos de clasificación binaria\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Dividir datos en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definir función para construir y entrenar modelos con diferentes funciones de activación\n",
        "def build_and_train_model(activation, solver):\n",
        "    model = Sequential([\n",
        "        Dense(100, activation=activation, input_shape=(X_train.shape[1],)),\n",
        "        Dense(50,activation=activation),\n",
        "        Dense(33, activation=activation),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=solver, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=10, verbose=0)\n",
        "    return model\n",
        "\n",
        "# Entrenar modelos con diferentes funciones de activación\n",
        "# https://keras.io/api/optimizers/\n",
        "relu_model = build_and_train_model('relu',\"adam\")\n",
        "sigmoid_model = build_and_train_model('sigmoid',\"adam\")\n",
        "tanh_model = build_and_train_model('tanh',\"adam\")\n",
        "relu_model_sgd =build_and_train_model('tanh',\"sgd\")\n",
        "\n",
        "# Evaluar modelos en conjunto de prueba\n",
        "relu_acc = accuracy_score(y_test, np.round(relu_model.predict(X_test)))\n",
        "sigmoid_acc = accuracy_score(y_test, np.round(sigmoid_model.predict(X_test)))\n",
        "tanh_acc = accuracy_score(y_test, np.round(tanh_model.predict(X_test)))\n",
        "relu_acc_sgd = accuracy_score(y_test, np.round(relu_model_sgd.predict(X_test)))\n",
        "\n",
        "# Imprimir resultados\n",
        "print(\"Accuracy using ReLU activation:\", relu_acc)\n",
        "print(\"Accuracy using Sigmoid activation:\", sigmoid_acc)\n",
        "print(\"Accuracy using Tanh activation:\", tanh_acc)\n",
        "print(\"Accuracy using ReLU activation with sgd:\", relu_acc_sgd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGA2flyG2X3j",
        "outputId": "08ffe433-f467-44be-bb36-c35200551e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 2ms/step\n",
            "7/7 [==============================] - 0s 2ms/step\n",
            "7/7 [==============================] - 0s 2ms/step\n",
            "Accuracy using ReLU activation: 0.85\n",
            "Accuracy using Sigmoid activation: 0.88\n",
            "Accuracy using Tanh activation: 0.875\n",
            "Accuracy using ReLU activation with sgd: 0.855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos un conjunto de datos de clasificación binaria y dividimos los datos en conjuntos de entrenamiento y prueba.\n",
        "\n",
        "Luego, definimos una función para construir y entrenar modelos con diferentes funciones de activación en la capa oculta. Entrenamos modelos utilizando las funciones de activación ReLU, Sigmoid y Tanh, y luego evaluamos el rendimiento de cada modelo en el conjunto de prueba utilizando la precisión como métrica."
      ],
      "metadata": {
        "id": "w7hJKGBb2jht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Lista de nombres de funciones de activación\n",
        "activations = ['ReLU', 'Sigmoid', 'Tanh']\n",
        "\n",
        "# Lista de precisiones obtenidas por cada modelo\n",
        "accuracies = [relu_acc, sigmoid_acc, tanh_acc]\n",
        "\n",
        "# Crear gráfico de barras\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(activations, accuracies, color=['blue', 'green', 'orange'])\n",
        "plt.xlabel('Activation Function')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Comparison of Activation Functions')\n",
        "plt.ylim(0, 1)  # Establecer límites en el eje y\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "PoDu9rSt2evs",
        "outputId": "6766082a-9f12-4d98-dac3-581c68257add"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcwdvTB3mA6k"
      },
      "source": [
        "# Mejora de nuestra red MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actualicemos la red MNIST para reflejar lo que has aprendido.**\n",
        "\n",
        "Teóricamente, la función tanh debería mejorar la activación de la capa oculta, y softmax debería mejorar la función de activación de la capa de salida. Cuando las probamos, de hecho alcanzan una puntuación más alta. Pero las cosas no siempre son tan sencillas como parecen.\n",
        "\n",
        "Tuve que hacer un par de ajustes para afinar la red adecuadamente con estas nuevas activaciones. Para $tanh$, tuve que reducir la desviación estándar de los pesos entrantes. Recuerda que inicializas los pesos aleatoriamente. np.random.random crea una matriz aleatoria con números repartidos aleatoriamente entre 0 y 1. Multiplicando por 0.2 y restando por 0.1, reescalas este rango aleatorio para que esté entre -0.1 y 0.1. Esto funcionó muy bien para relu pero es menos óptimo para tanh. a tanh le gusta tener una inicialización aleatoria más estrecha, así que la ajusté para que estuviera entre -0.01 y 0.01.\n",
        "\n",
        "También he eliminado el cálculo de error. Técnicamente, softmax se utiliza mejor con una función de error llamada entropía cruzada. Esta red calcula correctamente layer_2_delta para esta medida de error, pero como no hemos analizado por qué esta función de error es ventajosa, he eliminado las líneas para calcularla.\n",
        "\n",
        "Por último, como con casi todos los cambios realizados en una red neuronal, tuve que revisar el ajuste de alfa. Descubrí que se necesitaba un alfa mucho más alto para alcanzar una buena puntuación en 300 iteraciones. Y ¡voilá! Como era de esperar, la red alcanzó una mayor precisión en las pruebas, del 87%.\n"
      ],
      "metadata": {
        "id": "mukIkNUcnj65"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FUr0EqBmA6l",
        "outputId": "0643157f-3a1c-4a06-e703-7ba5f0fbd65b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "\n",
            "I:0 Test-Acc:0.394 Train-Acc:0.156\n",
            "I:10 Test-Acc:0.6867 Train-Acc:0.723\n",
            "I:20 Test-Acc:0.7025 Train-Acc:0.732\n",
            "I:30 Test-Acc:0.734 Train-Acc:0.763\n",
            "I:40 Test-Acc:0.7663 Train-Acc:0.794\n",
            "I:50 Test-Acc:0.7913 Train-Acc:0.819\n",
            "I:60 Test-Acc:0.8102 Train-Acc:0.849\n",
            "I:70 Test-Acc:0.8228 Train-Acc:0.864\n",
            "I:80 Test-Acc:0.831 Train-Acc:0.867\n",
            "I:90 Test-Acc:0.8364 Train-Acc:0.885\n",
            "I:100 Test-Acc:0.8407 Train-Acc:0.883\n",
            "I:110 Test-Acc:0.845 Train-Acc:0.891\n",
            "I:120 Test-Acc:0.8481 Train-Acc:0.901\n",
            "I:130 Test-Acc:0.8505 Train-Acc:0.901\n",
            "I:140 Test-Acc:0.8526 Train-Acc:0.905\n",
            "I:150 Test-Acc:0.8555 Train-Acc:0.914\n",
            "I:160 Test-Acc:0.8577 Train-Acc:0.925\n",
            "I:170 Test-Acc:0.8596 Train-Acc:0.918\n",
            "I:180 Test-Acc:0.8619 Train-Acc:0.933\n",
            "I:190 Test-Acc:0.863 Train-Acc:0.933\n",
            "I:200 Test-Acc:0.8642 Train-Acc:0.926\n",
            "I:210 Test-Acc:0.8653 Train-Acc:0.931\n",
            "I:220 Test-Acc:0.8668 Train-Acc:0.93\n",
            "I:230 Test-Acc:0.8672 Train-Acc:0.937\n",
            "I:240 Test-Acc:0.8681 Train-Acc:0.938\n",
            "I:250 Test-Acc:0.8687 Train-Acc:0.937\n",
            "I:260 Test-Acc:0.8684 Train-Acc:0.945\n",
            "I:270 Test-Acc:0.8703 Train-Acc:0.951\n",
            "I:280 Test-Acc:0.8699 Train-Acc:0.949\n",
            "I:290 Test-Acc:0.8701 Train-Acc:0.94"
          ]
        }
      ],
      "source": [
        "import numpy as np, sys\n",
        "np.random.seed(1)\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])\n",
        "\n",
        "one_hot_labels = np.zeros((len(labels),10))\n",
        "for i,l in enumerate(labels):\n",
        "    one_hot_labels[i][l] = 1\n",
        "labels = one_hot_labels\n",
        "\n",
        "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
        "test_labels = np.zeros((len(y_test),10))\n",
        "for i,l in enumerate(y_test):\n",
        "    test_labels[i][l] = 1\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh2deriv(output):\n",
        "    return 1 - (output ** 2)\n",
        "\n",
        "def softmax(x):\n",
        "    temp = np.exp(x)\n",
        "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
        "\n",
        "alpha, iterations, hidden_size = (2, 300, 100)\n",
        "pixels_per_image, num_labels = (784, 10)\n",
        "batch_size = 100\n",
        "\n",
        "weights_0_1 = 0.02*np.random.random((pixels_per_image,hidden_size))-0.01\n",
        "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
        "\n",
        "for j in range(iterations):\n",
        "    correct_cnt = 0\n",
        "    for i in range(int(len(images) / batch_size)):\n",
        "        batch_start, batch_end=((i * batch_size),((i+1)*batch_size))\n",
        "        layer_0 = images[batch_start:batch_end]\n",
        "        layer_1 = tanh(np.dot(layer_0,weights_0_1))\n",
        "        dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
        "        layer_1 *= dropout_mask * 2\n",
        "        layer_2 = softmax(np.dot(layer_1,weights_1_2))\n",
        "\n",
        "        for k in range(batch_size):\n",
        "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
        "\n",
        "        layer_2_delta = (labels[batch_start:batch_end]-layer_2) / (batch_size * layer_2.shape[0])\n",
        "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tanh2deriv(layer_1)\n",
        "        layer_1_delta *= dropout_mask\n",
        "\n",
        "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "\n",
        "    test_correct_cnt = 0\n",
        "\n",
        "    for i in range(len(test_images)):\n",
        "\n",
        "        layer_0 = test_images[i:i+1]\n",
        "        layer_1 = tanh(np.dot(layer_0,weights_0_1))\n",
        "        layer_2 = np.dot(layer_1,weights_1_2)\n",
        "\n",
        "        test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
        "    if(j % 10 == 0):\n",
        "        sys.stdout.write(\"\\n\"+ \\\n",
        "         \"I:\" + str(j) + \\\n",
        "         \" Test-Acc:\"+str(test_correct_cnt/float(len(test_images)))+\\\n",
        "         \" Train-Acc:\" + str(correct_cnt/float(len(images))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CBzWIQemA6m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mejora de nuestra red MNIST"
      ],
      "metadata": {
        "id": "JBF24XuApkBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reutilización de pesos en varios lugares**\n",
        "\n",
        "Si necesita detectar la misma característica en varios lugares utilice las mismas ponderaciones.\n",
        "\n",
        "El mayor reto de las redes neuronales es el de la sobreadaptación, cuando una red neuronal memoriza un conjunto de datos en lugar de aprender abstracciones útiles que generalicen a datos no vistos. En otras palabras, la red neuronal aprende a predecir basándose en el ruido del conjunto de datos en lugar de basarse en la señal fundamental (¿recuerdas la analogía sobre un tenedor incrustado en arcilla?).\n",
        "\n",
        "El sobreajuste suele deberse a que se dispone de más parámetros de los necesarios para aprender un conjunto de datos específico. En este caso, la red tiene tantos parámetros que puede memorizar todos los detalles del conjunto de datos de entrenamiento, en lugar de aprender abstracciones de alto nivel. Cuando las redes neuronales tienen muchos parámetros pero pocos ejemplos de entrenamiento, es difícil evitar el sobreajuste.)\n",
        "\n",
        "**La capa convolucional**\n",
        "\n",
        "Muchas capas lineales muy pequeñas se reutilizan en cada posición,\n",
        "en lugar de una única capa grande. La idea central de una capa convolucional es que, en lugar de tener una capa lineal grande y densa con una conexión desde cada entrada hasta cada salida, se tienen muchas capas lineales muy pequeñas, normalmente con menos de 25 entradas y una única salida, que se utilizan en cada posición de entrada. Cada minicapa se denomina núcleo convolucional, pero en realidad no es más que una pequeña capa lineal con un pequeño número de entradas y una única salida.\n"
      ],
      "metadata": {
        "id": "vEiBV6VpptxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementación en NUMPY"
      ],
      "metadata": {
        "id": "fWQP6-EhrWgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, sys\n",
        "np.random.seed(1)\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "images, labels = (x_train[0:1000].reshape(1000,28*28) / 255,\n",
        "                  y_train[0:1000])\n",
        "\n",
        "\n",
        "one_hot_labels = np.zeros((len(labels),10))\n",
        "for i,l in enumerate(labels):\n",
        "    one_hot_labels[i][l] = 1\n",
        "labels = one_hot_labels\n",
        "\n",
        "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
        "test_labels = np.zeros((len(y_test),10))\n",
        "for i,l in enumerate(y_test):\n",
        "    test_labels[i][l] = 1\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh2deriv(output):\n",
        "    return 1 - (output ** 2)\n",
        "\n",
        "def softmax(x):\n",
        "    temp = np.exp(x)\n",
        "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
        "\n",
        "alpha, iterations = (2, 300)\n",
        "pixels_per_image, num_labels = (784, 10)\n",
        "batch_size = 128\n",
        "\n",
        "input_rows = 28\n",
        "input_cols = 28\n",
        "\n",
        "kernel_rows = 3\n",
        "kernel_cols = 3\n",
        "num_kernels = 16\n",
        "\n",
        "hidden_size = ((input_rows - kernel_rows) *\n",
        "               (input_cols - kernel_cols)) * num_kernels\n",
        "\n",
        "# weights_0_1 = 0.02*np.random.random((pixels_per_image,hidden_size))-0.01\n",
        "kernels = 0.02*np.random.random((kernel_rows*kernel_cols,\n",
        "                                 num_kernels))-0.01\n",
        "\n",
        "weights_1_2 = 0.2*np.random.random((hidden_size,\n",
        "                                    num_labels)) - 0.1\n",
        "\n",
        "\n",
        "\n",
        "def get_image_section(layer,row_from, row_to, col_from, col_to):\n",
        "    section = layer[:,row_from:row_to,col_from:col_to]\n",
        "    return section.reshape(-1,1,row_to-row_from, col_to-col_from)\n",
        "\n",
        "for j in range(iterations):\n",
        "    correct_cnt = 0\n",
        "    for i in range(int(len(images) / batch_size)):\n",
        "        batch_start, batch_end=((i * batch_size),((i+1)*batch_size))\n",
        "        layer_0 = images[batch_start:batch_end]\n",
        "        layer_0 = layer_0.reshape(layer_0.shape[0],28,28)\n",
        "        layer_0.shape\n",
        "\n",
        "        sects = list()\n",
        "        for row_start in range(layer_0.shape[1]-kernel_rows):\n",
        "            for col_start in range(layer_0.shape[2] - kernel_cols):\n",
        "                sect = get_image_section(layer_0,\n",
        "                                         row_start,\n",
        "                                         row_start+kernel_rows,\n",
        "                                         col_start,\n",
        "                                         col_start+kernel_cols)\n",
        "                sects.append(sect)\n",
        "\n",
        "        expanded_input = np.concatenate(sects,axis=1)\n",
        "        es = expanded_input.shape\n",
        "        flattened_input = expanded_input.reshape(es[0]*es[1],-1)\n",
        "\n",
        "        kernel_output = flattened_input.dot(kernels)\n",
        "        layer_1 = tanh(kernel_output.reshape(es[0],-1))\n",
        "        dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
        "        layer_1 *= dropout_mask * 2\n",
        "        layer_2 = softmax(np.dot(layer_1,weights_1_2))\n",
        "\n",
        "        for k in range(batch_size):\n",
        "            labelset = labels[batch_start+k:batch_start+k+1]\n",
        "            _inc = int(np.argmax(layer_2[k:k+1]) ==\n",
        "                               np.argmax(labelset))\n",
        "            correct_cnt += _inc\n",
        "\n",
        "        layer_2_delta = (labels[batch_start:batch_end]-layer_2)\\\n",
        "                        / (batch_size * layer_2.shape[0])\n",
        "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * \\\n",
        "                        tanh2deriv(layer_1)\n",
        "        layer_1_delta *= dropout_mask\n",
        "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "        l1d_reshape = layer_1_delta.reshape(kernel_output.shape)\n",
        "        k_update = flattened_input.T.dot(l1d_reshape)\n",
        "        kernels -= alpha * k_update\n",
        "\n",
        "    test_correct_cnt = 0\n",
        "\n",
        "    for i in range(len(test_images)):\n",
        "\n",
        "        layer_0 = test_images[i:i+1]\n",
        "#         layer_1 = tanh(np.dot(layer_0,weights_0_1))\n",
        "        layer_0 = layer_0.reshape(layer_0.shape[0],28,28)\n",
        "        layer_0.shape\n",
        "\n",
        "        sects = list()\n",
        "        for row_start in range(layer_0.shape[1]-kernel_rows):\n",
        "            for col_start in range(layer_0.shape[2] - kernel_cols):\n",
        "                sect = get_image_section(layer_0,\n",
        "                                         row_start,\n",
        "                                         row_start+kernel_rows,\n",
        "                                         col_start,\n",
        "                                         col_start+kernel_cols)\n",
        "                sects.append(sect)\n",
        "\n",
        "        expanded_input = np.concatenate(sects,axis=1)\n",
        "        es = expanded_input.shape\n",
        "        flattened_input = expanded_input.reshape(es[0]*es[1],-1)\n",
        "\n",
        "        kernel_output = flattened_input.dot(kernels)\n",
        "        layer_1 = tanh(kernel_output.reshape(es[0],-1))\n",
        "        layer_2 = np.dot(layer_1,weights_1_2)\n",
        "\n",
        "        test_correct_cnt += int(np.argmax(layer_2) ==\n",
        "                                np.argmax(test_labels[i:i+1]))\n",
        "    if(j % 1 == 0):\n",
        "        sys.stdout.write(\"\\n\"+ \\\n",
        "         \"I:\" + str(j) + \\\n",
        "         \" Test-Acc:\"+str(test_correct_cnt/float(len(test_images)))+\\\n",
        "         \" Train-Acc:\" + str(correct_cnt/float(len(images))))"
      ],
      "metadata": {
        "id": "yZhf9Hb2plbZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "9de0a5b2-6b28-4df6-bb9c-ff81f4360f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I:0 Test-Acc:0.0288 Train-Acc:0.055\n",
            "I:1 Test-Acc:0.0273 Train-Acc:0.037\n",
            "I:2 Test-Acc:0.028 Train-Acc:0.037\n",
            "I:3 Test-Acc:0.0292 Train-Acc:0.04\n",
            "I:4 Test-Acc:0.0339 Train-Acc:0.046\n",
            "I:5 Test-Acc:0.0478 Train-Acc:0.068"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1a641c1bfd76>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_start\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mkernel_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol_start\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mkernel_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 sect = get_image_section(layer_0,\n\u001b[0m\u001b[1;32m    116\u001b[0m                                          \u001b[0mrow_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                                          \u001b[0mrow_start\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mkernel_rows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-1a641c1bfd76>\u001b[0m in \u001b[0;36mget_image_section\u001b[0;34m(layer, row_from, row_to, col_from, col_to)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_image_section\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow_from\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_from\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow_from\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrow_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_from\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcol_to\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow_to\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrow_from\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_to\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcol_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como puedes ver, cambiar la primera capa de la red por una capa convolucional da otros pocos puntos porcentuales en la reducción de errores. La salida de la capa convolucional (kernel_output) es a su vez una serie de imágenes bidimensionales (la salida de cada kernel en cada posición de entrada).\n",
        "\n",
        "La mayoría de los usos de las capas convolucionales apilan varias capas unas sobre otras, de forma que cada capa convolucional trata la anterior como una imagen de entrada. (Siéntete libre de hacer esto como proyecto personal; aumentará aún más la precisión).\n",
        "\n",
        "Las capas convolucionales apiladas son uno de los principales avances que permitieron crear redes neuronales muy profundas (y, por extensión, la popularización de la expresión aprendizaje profundo). Nunca se insistirá lo suficiente en que este invento marcó un hito en este campo; sin él, es posible que aún estuviéramos en el anterior invierno de la IA, incluso en el momento de escribir estas líneas.\n"
      ],
      "metadata": {
        "id": "Ri73KJHcq4A0"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}